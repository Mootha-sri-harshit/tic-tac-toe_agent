{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODx5QGdZBtmDfLV7m+WZQI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mootha-sri-harshit/tic-tac-toe_agent/blob/main/tic_tac_toe_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "\n",
        "# ==================== Tic-Tac-Toe Environment ====================\n",
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        self.board = [' '] * 9\n",
        "        self.win_combinations = [\n",
        "            (0,1,2), (3,4,5), (6,7,8), (0,3,6),\n",
        "            (1,4,7), (2,5,8), (0,4,8), (2,4,6)\n",
        "        ]\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = [' '] * 9\n",
        "        return tuple(self.board)\n",
        "\n",
        "    def available_moves(self):\n",
        "        return [i for i, spot in enumerate(self.board) if spot == ' ']\n",
        "\n",
        "    def make_move(self, position, player):\n",
        "        if self.board[position] == ' ':\n",
        "            self.board[position] = player\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def check_winner(self):\n",
        "        for combo in self.win_combinations:\n",
        "            a, b, c = combo\n",
        "            if self.board[a] == self.board[b] == self.board[c] != ' ':\n",
        "                return self.board[a]\n",
        "        return \"Draw\" if ' ' not in self.board else None\n",
        "\n",
        "    def print_board(self):\n",
        "        for i in range(0, 9, 3):\n",
        "            print(f\"{self.board[i]} | {self.board[i+1]} | {self.board[i+2]}\")\n",
        "        print(\"-\" * 9)\n",
        "\n",
        "# ==================== Noisy Network Layer ====================\n",
        "class NoisyLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, std_init=0.4):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.std_init = std_init\n",
        "\n",
        "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.weight_sigma = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.register_buffer('weight_epsilon', torch.Tensor(out_features, in_features))\n",
        "\n",
        "        self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n",
        "        self.bias_sigma = nn.Parameter(torch.Tensor(out_features))\n",
        "        self.register_buffer('bias_epsilon', torch.Tensor(out_features))\n",
        "\n",
        "        self.reset_parameters()\n",
        "        self.reset_noise()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        mu_range = 1 / np.sqrt(self.in_features)\n",
        "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
        "        self.weight_sigma.data.fill_(self.std_init / np.sqrt(self.in_features))\n",
        "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
        "        self.bias_sigma.data.fill_(self.std_init / np.sqrt(self.out_features))\n",
        "\n",
        "    def reset_noise(self):\n",
        "        epsilon_in = self._scale_noise(self.in_features)\n",
        "        epsilon_out = self._scale_noise(self.out_features)\n",
        "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
        "        self.bias_epsilon.copy_(epsilon_out)\n",
        "\n",
        "    def _scale_noise(self, size):\n",
        "        x = torch.randn(size)\n",
        "        return x.sign().mul(x.abs().sqrt())\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training:\n",
        "            return F.linear(x,\n",
        "                self.weight_mu + self.weight_sigma * self.weight_epsilon,\n",
        "                self.bias_mu + self.bias_sigma * self.bias_epsilon)\n",
        "        else:\n",
        "            return F.linear(x, self.weight_mu, self.bias_mu)\n",
        "\n",
        "# ==================== DQN Architecture ====================\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            NoisyLinear(9, 256),\n",
        "            nn.ReLU(),\n",
        "            NoisyLinear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            NoisyLinear(256, 9)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def reset_noise(self):\n",
        "        for layer in self.net:\n",
        "            if isinstance(layer, NoisyLinear):\n",
        "                layer.reset_noise()\n",
        "\n",
        "# ==================== DQN Agent ====================\n",
        "class DQNAgent:\n",
        "    def __init__(self):\n",
        "        self.policy_net = DQN()\n",
        "        self.target_net = DQN()\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=5e-4, weight_decay=1e-5)\n",
        "        self.memory = deque(maxlen=50000)\n",
        "        self.batch_size = 128\n",
        "        self.gamma = 0.97\n",
        "        self.train_step = 0\n",
        "\n",
        "    def get_state_tensor(self, state):\n",
        "        return torch.FloatTensor([\n",
        "            1 if cell == 'X' else -1 if cell == 'O' else 0\n",
        "            for cell in state\n",
        "        ])\n",
        "\n",
        "    def choose_action(self, state, available_moves):\n",
        "        state_tensor = self.get_state_tensor(state).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            q_values = self.policy_net(state_tensor)\n",
        "\n",
        "        # Boltzmann exploration with dynamic temperature\n",
        "        temp = max(0.5, 2.0 - self.train_step/10000)\n",
        "        mask = torch.full((9,), -np.inf)\n",
        "        mask[available_moves] = 0\n",
        "        probs = F.softmax((q_values + mask) / temp, dim=1)\n",
        "        return torch.multinomial(probs, 1).item()\n",
        "\n",
        "    def store_experience(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def update_model(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = torch.stack([self.get_state_tensor(s) for s in states])\n",
        "        actions = torch.tensor(actions)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "        next_states = torch.stack([self.get_state_tensor(s) for s in next_states])\n",
        "        dones = torch.tensor(dones, dtype=torch.bool)\n",
        "\n",
        "        # Double DQN with target network\n",
        "        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
        "        with torch.no_grad():\n",
        "            next_actions = self.policy_net(next_states).argmax(1)\n",
        "            next_q = self.target_net(next_states).gather(1, next_actions.unsqueeze(1))\n",
        "\n",
        "        target_q = rewards + (1 - dones.float()) * self.gamma * next_q.squeeze()\n",
        "        loss = F.smooth_l1_loss(current_q.squeeze(), target_q)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 5.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update target network and reset noise\n",
        "        self.train_step += 1\n",
        "        if self.train_step % 250 == 0:\n",
        "            for target_param, policy_param in zip(self.target_net.parameters(),\n",
        "                                                 self.policy_net.parameters()):\n",
        "                target_param.data.copy_(0.005*policy_param.data + 0.995*target_param.data)\n",
        "\n",
        "        self.policy_net.reset_noise()\n",
        "        self.target_net.reset_noise()\n",
        "\n",
        "    def save(self, path):\n",
        "        torch.save({\n",
        "            'policy_state': self.policy_net.state_dict(),\n",
        "            'target_state': self.target_net.state_dict(),\n",
        "            'optimizer': self.optimizer.state_dict(),\n",
        "            'train_step': self.train_step,\n",
        "            'memory': list(self.memory)\n",
        "        }, path)\n",
        "\n",
        "    def load(self, path):\n",
        "        checkpoint = torch.load(path)\n",
        "        self.policy_net.load_state_dict(checkpoint['policy_state'])\n",
        "        self.target_net.load_state_dict(checkpoint['target_state'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        self.train_step = checkpoint['train_step']\n",
        "        self.memory = deque(checkpoint['memory'], maxlen=50000)\n",
        "\n",
        "# ==================== Training and Evaluation ====================\n",
        "def find_blocking_move(env, player):\n",
        "    opponent = 'X' if player == 'O' else 'O'\n",
        "    for combo in env.win_combinations:\n",
        "        cells = [env.board[i] for i in combo]\n",
        "        if cells.count(opponent) == 2 and cells.count(' ') == 1:\n",
        "            return combo[cells.index(' ')]\n",
        "    return None\n",
        "\n",
        "def train_agent(episodes=10000):\n",
        "    env = TicTacToe()\n",
        "    agent = DQNAgent()\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Agent's move (X)\n",
        "            avail_moves = env.available_moves()\n",
        "            action = agent.choose_action(state, avail_moves)\n",
        "            env.make_move(action, 'X')\n",
        "            next_state = env.board.copy()\n",
        "            winner = env.check_winner()\n",
        "\n",
        "            # Reward shaping\n",
        "            if winner == 'X':\n",
        "                reward = 1 + 0.2*(9 - len(avail_moves))\n",
        "                done = True\n",
        "            elif winner == 'Draw':\n",
        "                reward = 0.5\n",
        "                done = True\n",
        "            else:\n",
        "                # Mixed opponent strategy\n",
        "                opp_moves = env.available_moves()\n",
        "                if np.random.rand() < 0.7:\n",
        "                    opp_action = random.choice(opp_moves) if opp_moves else None\n",
        "                else:\n",
        "                    opp_action = find_blocking_move(env, 'O')\n",
        "\n",
        "                if opp_action is not None:\n",
        "                    env.make_move(opp_action, 'O')\n",
        "                    winner = env.check_winner()\n",
        "                    next_state = env.board.copy()\n",
        "\n",
        "                    if winner == 'O':\n",
        "                        reward = -1 - 0.2*(9 - len(opp_moves))\n",
        "                        done = True\n",
        "                    elif winner == 'Draw':\n",
        "                        reward = 0.3\n",
        "                        done = True\n",
        "                    else:\n",
        "                        reward = -0.05 * (9 - len(opp_moves))\n",
        "                else:\n",
        "                    reward = -0.2\n",
        "\n",
        "            agent.store_experience(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "\n",
        "            if len(agent.memory) > agent.batch_size:\n",
        "                agent.update_model()\n",
        "\n",
        "        # Save model periodically\n",
        "        if episode % 500 == 0:\n",
        "            agent.save(\"dqn_tic_tac_toe.pth\")\n",
        "            print(f\"Episode {episode} | Model Saved\")\n",
        "\n",
        "    return agent\n",
        "\n",
        "def play_vs_ai(agent_path=\"dqn_tic_tac_toe.pth\"):\n",
        "    agent = DQNAgent()\n",
        "    agent.load(agent_path)\n",
        "\n",
        "    env = TicTacToe()\n",
        "    human = input(\"Choose X or O: \").upper().strip()\n",
        "    while human not in ['X', 'O']:\n",
        "        human = input(\"Invalid! Choose X/O: \").upper().strip()\n",
        "\n",
        "    ai = 'O' if human == 'X' else 'X'\n",
        "    current_player = 'X'\n",
        "    state = env.reset()\n",
        "\n",
        "    while True:\n",
        "        env.print_board()\n",
        "        if current_player == human:\n",
        "            try:\n",
        "                move = int(input(\"Your move (0-8): \"))\n",
        "                if move not in env.available_moves():\n",
        "                    print(\"Invalid move!\")\n",
        "                    continue\n",
        "            except ValueError:\n",
        "                print(\"Enter a number 0-8\")\n",
        "                continue\n",
        "        else:\n",
        "            move = agent.choose_action(state, env.available_moves())\n",
        "            print(f\"AI chooses: {move}\")\n",
        "\n",
        "        env.make_move(move, current_player)\n",
        "        winner = env.check_winner()\n",
        "\n",
        "        if winner:\n",
        "            env.print_board()\n",
        "            print(f\"Game Over! Winner: {winner}\")\n",
        "            break\n",
        "\n",
        "        current_player = 'O' if current_player == 'X' else 'X'\n",
        "        state = env.board.copy()\n",
        "\n",
        "# ==================== Main Execution ====================\n",
        "if __name__ == \"__main__\":\n",
        "    # Train the agent (start with 1000 episodes for testing)\n",
        "    trained_agent = train_agent(episodes=5000)\n",
        "\n",
        "    # Play against the AI\n",
        "    play_vs_ai()\n"
      ],
      "metadata": {
        "id": "GHqRmePesGZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c602b69-d839-4946-f15d-1b0e2b5e69d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0 | Model Saved\n",
            "Episode 500 | Model Saved\n",
            "Episode 1000 | Model Saved\n",
            "Episode 1500 | Model Saved\n",
            "Episode 2000 | Model Saved\n",
            "Episode 2500 | Model Saved\n",
            "Episode 3000 | Model Saved\n",
            "Episode 3500 | Model Saved\n",
            "Episode 4000 | Model Saved\n",
            "Episode 4500 | Model Saved\n",
            "Choose X or O: x\n",
            "  |   |  \n",
            "  |   |  \n",
            "  |   |  \n",
            "---------\n",
            "Your move (0-8): 4\n",
            "  |   |  \n",
            "  | X |  \n",
            "  |   |  \n",
            "---------\n",
            "AI chooses: 0\n",
            "O |   |  \n",
            "  | X |  \n",
            "  |   |  \n",
            "---------\n",
            "Your move (0-8): 1\n",
            "O | X |  \n",
            "  | X |  \n",
            "  |   |  \n",
            "---------\n",
            "AI chooses: 7\n",
            "O | X |  \n",
            "  | X |  \n",
            "  | O |  \n",
            "---------\n",
            "Your move (0-8): 3\n",
            "O | X |  \n",
            "X | X |  \n",
            "  | O |  \n",
            "---------\n",
            "AI chooses: 5\n",
            "O | X |  \n",
            "X | X | O\n",
            "  | O |  \n",
            "---------\n",
            "Your move (0-8): 2\n",
            "O | X | X\n",
            "X | X | O\n",
            "  | O |  \n",
            "---------\n",
            "AI chooses: 6\n",
            "O | X | X\n",
            "X | X | O\n",
            "O | O |  \n",
            "---------\n",
            "Your move (0-8): 8\n",
            "O | X | X\n",
            "X | X | O\n",
            "O | O | X\n",
            "---------\n",
            "Game Over! Winner: Draw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "\n",
        "# ==================== Tic-Tac-Toe Environment ====================\n",
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        self.board = [' '] * 9\n",
        "        self.win_combinations = [\n",
        "            (0,1,2), (3,4,5), (6,7,8), (0,3,6),\n",
        "            (1,4,7), (2,5,8), (0,4,8), (2,4,6)\n",
        "        ]\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = [' '] * 9\n",
        "        return tuple(self.board)\n",
        "\n",
        "    def available_moves(self):\n",
        "        return [i for i, spot in enumerate(self.board) if spot == ' ']\n",
        "\n",
        "    def make_move(self, position, player):\n",
        "        if self.board[position] == ' ':\n",
        "            self.board[position] = player\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def check_winner(self):\n",
        "        for combo in self.win_combinations:\n",
        "            a, b, c = combo\n",
        "            if self.board[a] == self.board[b] == self.board[c] != ' ':\n",
        "                return self.board[a]\n",
        "        return \"Draw\" if ' ' not in self.board else None\n",
        "\n",
        "    def print_board(self):\n",
        "        for i in range(0, 9, 3):\n",
        "            print(f\"{self.board[i]} | {self.board[i+1]} | {self.board[i+2]}\")\n",
        "        print(\"-\" * 9)\n",
        "\n",
        "# ==================== Noisy Network Layer ====================\n",
        "class NoisyLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, std_init=0.4):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.std_init = std_init\n",
        "\n",
        "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.weight_sigma = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.register_buffer('weight_epsilon', torch.Tensor(out_features, in_features))\n",
        "\n",
        "        self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n",
        "        self.bias_sigma = nn.Parameter(torch.Tensor(out_features))\n",
        "        self.register_buffer('bias_epsilon', torch.Tensor(out_features))\n",
        "\n",
        "        self.reset_parameters()\n",
        "        self.reset_noise()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        mu_range = 1 / np.sqrt(self.in_features)\n",
        "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
        "        self.weight_sigma.data.fill_(self.std_init / np.sqrt(self.in_features))\n",
        "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
        "        self.bias_sigma.data.fill_(self.std_init / np.sqrt(self.out_features))\n",
        "\n",
        "    def reset_noise(self):\n",
        "        epsilon_in = self._scale_noise(self.in_features)\n",
        "        epsilon_out = self._scale_noise(self.out_features)\n",
        "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
        "        self.bias_epsilon.copy_(epsilon_out)\n",
        "\n",
        "    def _scale_noise(self, size):\n",
        "        x = torch.randn(size)\n",
        "        return x.sign().mul(x.abs().sqrt())\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training:\n",
        "            return F.linear(x,\n",
        "                self.weight_mu + self.weight_sigma * self.weight_epsilon,\n",
        "                self.bias_mu + self.bias_sigma * self.bias_epsilon)\n",
        "        else:\n",
        "            return F.linear(x, self.weight_mu, self.bias_mu)\n",
        "\n",
        "# ==================== DQN Architecture ====================\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            NoisyLinear(9, 256),\n",
        "            nn.ReLU(),\n",
        "            NoisyLinear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            NoisyLinear(256, 9)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def reset_noise(self):\n",
        "        for layer in self.net:\n",
        "            if isinstance(layer, NoisyLinear):\n",
        "                layer.reset_noise()\n",
        "\n",
        "# ==================== DQN Agent ====================\n",
        "class DQNAgent:\n",
        "    def __init__(self):\n",
        "        self.policy_net = DQN()\n",
        "        self.target_net = DQN()\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=5e-4, weight_decay=1e-5)\n",
        "        self.memory = deque(maxlen=50000)\n",
        "        self.batch_size = 128\n",
        "        self.gamma = 0.97\n",
        "        self.train_step = 0\n",
        "\n",
        "    def get_state_tensor(self, state):\n",
        "        return torch.FloatTensor([\n",
        "            1 if cell == 'X' else -1 if cell == 'O' else 0\n",
        "            for cell in state\n",
        "        ])\n",
        "\n",
        "    def choose_action(self, state, available_moves):\n",
        "        state_tensor = self.get_state_tensor(state).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            q_values = self.policy_net(state_tensor)\n",
        "        temp = max(0.5, 2.0 - self.train_step/10000)\n",
        "        mask = torch.full((9,), -np.inf)\n",
        "        mask[available_moves] = 0\n",
        "        probs = F.softmax((q_values + mask) / temp, dim=1)\n",
        "        return torch.multinomial(probs, 1).item()\n",
        "\n",
        "    def save(self, path):\n",
        "        torch.save({\n",
        "            'policy_state': self.policy_net.state_dict(),\n",
        "            'target_state': self.target_net.state_dict(),\n",
        "            'optimizer': self.optimizer.state_dict(),\n",
        "            'train_step': self.train_step,\n",
        "            'memory': list(self.memory)\n",
        "        }, path)\n",
        "\n",
        "    def load(self, path):\n",
        "        checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
        "        self.policy_net.load_state_dict(checkpoint['policy_state'])\n",
        "        self.target_net.load_state_dict(checkpoint['target_state'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        self.train_step = checkpoint['train_step']\n",
        "        self.memory = deque(checkpoint['memory'], maxlen=50000)\n",
        "\n",
        "# ==================== Play Against AI ====================\n",
        "def play_vs_ai(agent_path=\"dqn_tic_tac_toe.pth\"):\n",
        "    agent = DQNAgent()\n",
        "    agent.load(agent_path)\n",
        "\n",
        "    env = TicTacToe()\n",
        "    human = input(\"Choose X or O: \").upper().strip()\n",
        "    while human not in ['X', 'O']:\n",
        "        human = input(\"Invalid! Choose X/O: \").upper().strip()\n",
        "\n",
        "    ai = 'O' if human == 'X' else 'X'\n",
        "    current_player = 'X'\n",
        "    state = env.reset()\n",
        "\n",
        "    while True:\n",
        "        env.print_board()\n",
        "        if current_player == human:\n",
        "            try:\n",
        "                move = int(input(\"Your move (0-8): \"))\n",
        "                if move not in env.available_moves():\n",
        "                    print(\"Invalid move!\")\n",
        "                    continue\n",
        "            except ValueError:\n",
        "                print(\"Enter a number 0-8\")\n",
        "                continue\n",
        "        else:\n",
        "            move = agent.choose_action(state, env.available_moves())\n",
        "            print(f\"AI chooses: {move}\")\n",
        "\n",
        "        env.make_move(move, current_player)\n",
        "        winner = env.check_winner()\n",
        "\n",
        "        if winner:\n",
        "            env.print_board()\n",
        "            print(f\"Game Over! Winner: {winner}\")\n",
        "            break\n",
        "\n",
        "        current_player = 'O' if current_player == 'X' else 'X'\n",
        "        state = env.board.copy()\n",
        "\n",
        "# ==================== Main Execution ====================\n",
        "play_vs_ai(\"dqn_tic_tac_toe.pth\")\n"
      ],
      "metadata": {
        "id": "HN2t6kkjxN1y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58dcdf11-baf3-4146-fd4d-2c9903bdc55b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose X or O: x\n",
            "  |   |  \n",
            "  |   |  \n",
            "  |   |  \n",
            "---------\n",
            "Your move (0-8): 1\n",
            "  | X |  \n",
            "  |   |  \n",
            "  |   |  \n",
            "---------\n",
            "AI chooses: 0\n",
            "O | X |  \n",
            "  |   |  \n",
            "  |   |  \n",
            "---------\n",
            "Your move (0-8): 4\n",
            "O | X |  \n",
            "  | X |  \n",
            "  |   |  \n",
            "---------\n",
            "AI chooses: 7\n",
            "O | X |  \n",
            "  | X |  \n",
            "  | O |  \n",
            "---------\n",
            "Your move (0-8): 2\n",
            "O | X | X\n",
            "  | X |  \n",
            "  | O |  \n",
            "---------\n",
            "AI chooses: 6\n",
            "O | X | X\n",
            "  | X |  \n",
            "O | O |  \n",
            "---------\n",
            "Your move (0-8): 3\n",
            "O | X | X\n",
            "X | X |  \n",
            "O | O |  \n",
            "---------\n",
            "AI chooses: 5\n",
            "O | X | X\n",
            "X | X | O\n",
            "O | O |  \n",
            "---------\n",
            "Your move (0-8): 8\n",
            "O | X | X\n",
            "X | X | O\n",
            "O | O | X\n",
            "---------\n",
            "Game Over! Winner: Draw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FM-O0xqo_hQi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}